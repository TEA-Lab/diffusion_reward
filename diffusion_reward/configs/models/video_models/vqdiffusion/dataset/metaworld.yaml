exp_name: metaworld

# change from o4
model:
  target: diffusion_reward.models.video_models.vqdiffusion.modeling.models.frame_conditional_dalle.FC_DALLE
  params:
    content_info: {key: image}
    condition_info: {key: frame}
    frame_skip: 1
    content_codec_config: 
      target: diffusion_reward.models.video_models.vqdiffusion.modeling.codecs.image_codec.vqgan.MiniVQGAN
      params:
        args: {latent_dim: 64, device: 'cuda', image_channels: 3, num_codebook_vectors: 1024, beta: 0.25, channels: [128, 128, 256, 256], resolution: 64, latent_size: 8}
        trainable: False
        token_shape: [8, 8]
        #config_path: 'OUTPUT/pretrained_model/taming_dvae/vqgan_imagenet_f16_16384.yaml'
        ckpt_path: /exp_local/codec_models/vqgan/metaworld/results/checkpoints/vqgan.pt
        # num_tokens: 16384
        # quantize_number: 974
        # mapping_path: './help_folder/statistics/taming_vqvae_974.pt'
        # return_logits: True
    diffusion_config:      
      target: diffusion_reward.models.video_models.vqdiffusion.modeling.transformers.diffusion_transformer.DiffusionTransformer
      params:
        diffusion_step: 100
        alpha_init_type: 'alpha1'        
        auxiliary_loss_weight: 1.0e-3
        adaptive_auxiliary_loss: True
        mask_weight: [1, 1]    # the loss weight on mask region and non-mask region

        transformer_config:
          target: diffusion_reward.models.video_models.vqdiffusion.modeling.transformers.transformer_utils.Text2ImageTransformer
          params:
            diffusion_step: ???
            content_emb_config: ???
            attn_type: 'selfcross'
            n_layer: 16
            condition_seq_len: 128    ###### 77 for clip and 256 for dalle
            content_seq_len: 64  # 32 x 32
            content_spatial_size: [8, 8]
            n_embd: 128 # the dim of embedding dims
            condition_dim: 1024
            n_head: 16
            attn_pdrop: 0.0
            resid_pdrop: 0.0
            block_activate: GELU2
            timestep_type: 'adalayernorm'    # adainsnorm or adalayernorm and abs
            mlp_hidden_times: 2
            mlp_type: 'conv_mlp'
        condition_emb_config:
          target: diffusion_reward.models.video_models.vqdiffusion.modeling.embeddings.frame_embedding.FrameEmbedding
          params:
            num_embed: 1024 # 
            embed_dim: 1024
            identity: false
            trainable: true
            num_cond_frames: 2
        content_emb_config:
          target: diffusion_reward.models.video_models.vqdiffusion.modeling.embeddings.dalle_mask_image_embedding.DalleMaskImageEmbedding
          params:
            num_embed: 1024
            spatial_size: [8, 8]
            embed_dim: 128
            trainable: True
            pos_emb_type: embedding

dataloader:
  data_root: "/video_dataset/metaworld/"
  batch_size: 4
  num_workers: 4
  train_datasets: # a list of configures, so we can combine several schedulers
    - target: diffusion_reward.models.video_models.vqdiffusion.data.dataset.VideoDataset
      params:
        data_root: ${dataloader.data_root}
        phase: train                                        
        frame_skip: ${model.params.frame_skip}
        frames_per_sample: 3

  validation_datasets:
    - target: diffusion_reward.models.video_models.vqdiffusion.data.dataset.VideoDataset
      params:
        data_root: ${dataloader.data_root}
        phase: test
        frame_skip: ${model.params.frame_skip}
        frames_per_sample: 3